from FPSim2 import FPSim2Engine
import numpy as np
import argparse
import duckdb
import pandas as pd
from pathlib import Path
from dataclasses import dataclass
from typing import Sequence
from utils import convert_hac_to_mw, convert_mw_to_hac
from functools import partial
import datamol as dm


def parse_args():
    parser = argparse.ArgumentParser(description='Search similar SMILES')
    parser.add_argument('-db', '--db_name', type=str, help='the name of the FPSIM2 fingerprint database',  required=True)
    parser.add_argument('-m','--molecular_database', type=str, help='The folder path for the analogous search database', required=True, default='Molecular_database/no_stereo')
    parser.add_argument('-ms','--original_database', type=str, help='The folder path for the original database', required=True, default='Molecular_database/deduplicated')
    parser.add_argument('-q','--query_path', type=str, help='File for the query in .smi format', required=True)
    parser.add_argument('-k','--top_k', type=int, help='How many molecular to retrieve (it might return less because of the algorithm)', required=False, default=500)
    parser.add_argument('-t','--threshold', type=float, help='The similarity threshold, faster when higher but it might return less molecules than specified', required=False, default=0.7)
    parser.add_argument('-n','--num_workers', type=int, help='The number of workers for the FPSIM2 search', required=False, default=50)
    parser.add_argument('-i', '--index_file', type=str, help='The index file generated by deduplication',  required=True)
    parser.add_argument('-c', '--output_file', type=str, help='The csv file name for the query results',  required=False, default="query_results.csv")
    parser.add_argument('-o', '--on_disk', action='store_false', help='Whether to perform on disk operations',  required=False)
    parser.add_argument('-mw', '--mw_range', nargs="+", type=int, help="The MW upper and/or the lower limit whn searching SMILES", default=None, required=False)
    parser.add_argument('-hac', '--hac_limits', nargs="+", type=int, help="The HAC upper and/or lower limit when searching SMILES", default=None, required=False)
    parser.add_argument("-st", "--search_type", required=False, default="similarity", choices=("similarity", "substructure"), help="The type of search to perform")
    args = parser.parse_args()
    return args.db_name, args.molecular_database, args.index_file, args.top_k, args.threshold, args.num_workers, args.output_file, args.query_path, args.on_disk, args.hac_limits, args.mw_range, args.search_type, args.original_database


@dataclass
class FPSim2Query:
    db_name: str
    query: str | list[str] # has to be SMILES
    workers: int=4
    on_disk: bool=True
    
    @property
    def fpe(self) -> FPSim2Engine:
        return FPSim2Engine(self.db_name, in_memory_fps=False if self.on_disk else True)
    
    @property
    def queries(self) -> list[str]:
        if isinstance(self.query, str):
            return [self.query]
        return self.query

    def similarity_search(
        self,
        top_k: int=500,
        threshold: float = 0.7):
        """Perform similarity search using FPSIM2"""
        search, tanimoto = {}, {}

        method = (
        self.fpe.on_disk_top_k
        if self.on_disk
        else self.fpe.top_k
        )
        
        for que in self.queries:
            results = method(que, top_k=top_k, threshold=threshold, n_workers=self.workers)
            search[que] = [int(x[0]) for x in results]
            tanimoto[que] = pd.Series({int(x[0]): float(x[1]) for x in results}).rename("Tanimoto")
        return search, tanimoto

    def substructure_screenout(
        self):
        """Perform substructure search using FPSIM2"""
        
        search = {}
        
        method = (
        self.fpe.on_disk_substructure
        if self.on_disk
        else self.fpe.substructure
        )
        for que in self.queries:
            results = method(que, n_workers=self.workers)
            search[que] = [int(x[0]) for x in results]

        return search, None
    
    def _match_smiles(self, smi: str, query_mol: dm.Mol) -> bool:
        mol = dm.to_mol(smi)
        return mol.HasSubstructMatch(query_mol)
    
    def match_substructure(self,
                           df_dict: dict[str, pd.DataFrame],
                           ) -> dict[str, pd.DataFrame]:
        """Perform substructure matching to filter false positives"""
        subs = {}
        for que in self.queries:
            query_mol = dm.to_mol(que)
            mask = dm.parallelized(partial(self._match_smiles, query_mol=query_mol), df_dict[que]["SMILES"], n_jobs=self.workers, progress=False, scheduler="threads")
            subs[que] = df_dict[que][mask]
            
        return subs


@dataclass
class RetrieveSmiles:
    """
    Retrieve smiles based on indexes from FPSIM2 search results and filter based on HAC or molecular weight range.
    
    Parameters
    -----------
    index_file: str
        The index file generated by deduplication which tells the size of each HAC
    database_path: str
        The folder path for the analoguous search database
    mw_range: Iterable[float] | None = None
        The range of molecular weights to filter. The first number is the lower bound and the second is the upper bound
    hac_limits: Iterable[int] | None = None
        The range of HACs to filter. The first number is the lower bound and the second is the upper bound.
    
    Only one of mw_range or hac_limits can be specified
    """
    index_file: str
    database_path: str
    mw_range: Sequence[float|int] | None = None
    hac_limits: Sequence[int] | None = None
    
    @property
    def upper(self) -> int | None:
        if self.hac_limits is not None and self.mw_range is not None:
            raise ValueError("Only one of mw_range or hac_limits can be specified")
        elif self.hac_limits is not None:
            return self.hac_limits[0]
        elif self.mw_range is not None:
            return convert_mw_to_hac(self.mw_range[0])
        else:
            return None
        
    @property
    def lower(self) -> int | None:
        if self.hac_limits is not None and len(self.hac_limits) == 2:
            return self.hac_limits[1]
        elif self.mw_range is not None and len(self.mw_range[1]) == 2:
            return convert_mw_to_hac(self.mw_range[1])
        else:
            return None
    

    def _filter_hacs(self, hac: int):
        u, l = True, True
        if self.upper is not None:
            u =  hac <= self.upper
        if self.lower is not None:
            l = hac >= self.lower                            
        return all([u, l])
        

    def find_hac_by_index(self, search_results: dict[str, list[int]]) -> dict[str, list[int]]:
        """
        Given the indices of the search results find in which HAC the molecule is found and reduce the scopre when
        retrieving the SMILES strings
        """
        with open(self.index_file, "r") as st:
            lines = {int(x.strip().split("#")[-1]): int(x.split("#")[0].strip("HAC")) for x in st.readlines()}
        index_dict = {}
        bounds = np.array(sorted(lines.keys()))
        for query, result in search_results.items():
            index = sorted(result)
            i = np.unique(np.searchsorted(bounds, index, side="right"))
            hacs = [lines.get(x) for x in bounds[i]]
            index_dict[query] = hacs
        return index_dict

    def convert_hac_topath(self,
        hac_dict: dict[str, list[int]], 
        ) -> dict[str, list[str]]:
        """
        Convert hac to parquet paths for the duck db read_parquet
        """
        parquet_paths = {}
        for query, hacs in hac_dict.items():
            hacs = filter(self._filter_hacs, hacs)
            parquet_paths[query] = [f"{self.database_path}/HAC_{hac}/*.parquet" for hac in hacs]
        return parquet_paths

    def retrieve_smiles(self,
        indices: list[int],
        parquet_path: list[str],
        ) -> pd.DataFrame:
        """ 
        Retrieve the SMILES from the databases and convert them into Dataframes
        """
        db_con = duckdb.connect()
        res = db_con.execute(f"ID, SELECT num_ID, nostereo_SMILES FROM read_parquet($path) WHERE num_ID IN $indices)", {"path": parquet_path, "indices": indices}).df()
        return res

    def batch_retrieve(
        self,
        search_result: dict[str, list[int]], 
        parquet_paths: dict[str, list[str]],
        more_data: dict[str, pd.Series] | None = None,
        ) -> dict[str, pd.DataFrame]:
        
        #extract the index from the FPSIM2 results and generate the combined path and indices
        """
        Batch retrieve SMILES and db_id from the database using the results of FPSIM2 search and the parquet file paths
        """

        index_dict = {query: sorted(ind) for query, ind in search_result.items()}
        parquet = set()
        index = set()
        for i in parquet_paths.values():
            parquet.update(i)
        for i in index_dict.values():
            index.update(i)
            
        result = {}
        # connect to database and search using the combined indices and parquet files   
        res = self.retrieve_smiles(sorted(index), list(parquet))
        for query, index in index_dict.items():
            dat = res[res["num_ID"].isin(index)]
            result[query] = pd.concat([dat, more_data[query]], axis=1) if more_data is not None else dat
        
        return result
    
    def run(self, 
        search_results: dict[str, list[int]],
        more_data: dict[str, pd.Series] | None = None,
        ) -> pd.DataFrame:
        """
        A convenient function to run the similarity search
        """
        hac_dict = self.find_hac_by_index(search_results)
        parquet_paths = self.convert_hac_topath(hac_dict)
        smiles = self.batch_retrieve(search_results, parquet_paths, more_data)
        return smiles

@dataclass
class RetrieveIsomers:
    original_database: str
    
    def retrieve_isomers(self, 
                         smiles_list: list[str],
                         parquet_path: str
                         ) -> dict[str, pd.DataFrame]:
        """
        Retrieve isomers for each query from the original database
        """
        db_con = duckdb.connect()
        res = db_con.execute(f"SELECT ID, SMILES, nostereo_SMILES, db_id FROM read_parquet($path) WHERE nostereo_SMILES IN $smiles", {"path": parquet_path, "smiles": smiles_list}).df()
        return res
        
    def bacth_retrieve_isomers(self,
                                smiles_dict: dict[str, pd.DataFrame],
                                ) -> dict[str, pd.DataFrame]:
        """
        Batch retrieve isomers for each query from the original database
        """
        smiles_lits = set()
        df = pd.concat(smiles_dict.values())
        smiles_list.update(df["nostereo_SMILES"].tolist())
        smiles_list = list(smiles_lits)
        hacs = [dm.to_mol(smi).GetNumHeavyAtoms() for smi in smiles_list]
        parquet_paths = [f"{self.original_database}/HAC_{hac}/*.parquet" for hac in set(hacs)]
        
        res = self.retrieve_isomers(smiles_list, parquet_paths)
        result = {}
        for query, df in smiles_dict.items():
            isomers = res[res["nostereo_SMILES"].isin(df["nostereo_SMILES"])]
            result[query] = isomers.drop(["nostereo_SMILES"], axis=1)
        
        return result

def main():
    db_name, molecular_database, index_file, top_k, threshold, num_workers, output_file, query_path, on_disk,hac_limits, mw_range, search_type, original_database = parse_args()
   
    with open(query_path) as w:
        query = [x.strip() for x in w.readlines()]
    fp = FPSim2Query(query=query, db_name=db_name, workers=num_workers, on_disk=on_disk)
    search = {"similarity": partial(fp.similarity_search, top_k=top_k, threshold=threshold), 
              "substructure": fp.substructure_screenout}    
    
    search_results, tanimoto = search[search_type]()
    
    retrieve = RetrieveSmiles(index_file, molecular_database, mw_range, hac_limits)
    smiles = retrieve.run(search_results, more_data=tanimoto)
    if search_type == "substructure":
        smiles = fp.match_substructure(smiles)
    retriever_isomers = RetrieveIsomers(original_database=original_database)
    smiles = retriever_isomers.bacth_retrieve_isomers(smiles)
    Path(output_file).parents.mkdir(parents=True, exist_ok=True)
    pd.concat(smiles).to_csv(output_file)
    
    
if __name__ == "__main__":
    # Run this if this file is executed from command line but not if is imported as API
    main()