from FPSim2 import FPSim2Engine
import numpy as np
import argparse
import duckdb
import pandas as pd
from pathlib import Path
from dataclasses import dataclass, field
from typing import Sequence
from utils import convert_hac_to_mw, convert_mw_to_hac
from functools import partial
from collections import defaultdict
import datamol as dm
import json
import os
import gc
import pickle as pk


def parse_args():
    parser = argparse.ArgumentParser(description='Search similar SMILES')
    parser.add_argument('db_name', type=str, help='the name of the FPSIM2 fingerprint database',  required=True)
    parser.add_argument('-m','--molecular_database', type=str, help='The folder path for the database where the similes without stereoisomer information is stored', required=True, default='Molecular_database/deduplicate_nostereo')
    parser.add_argument('-ms','--original_database', type=str, help='The folder path for the original database', required=True, default='Molecular_database/deduplicate_canonical')
    parser.add_argument('-q','--query_path', type=str, help='File for the query in .smi format', required=True)
    parser.add_argument('-k','--top_k', type=int, help='How many molecular to retrieve (it might return less because of the algorithm)', required=False, default=500)
    parser.add_argument('-t','--threshold', type=float, help='The similarity threshold, faster when higher but it might return less molecules than specified', required=False, default=0.7)
    parser.add_argument('-n','--num_workers', type=int, help='The number of workers for the FPSIM2 search', required=False, default=50)
    parser.add_argument('-i', '--index_file', type=str, help='The index file generated by deduplication',  required=True)
    parser.add_argument('-c', '--output_file', type=str, help='The csv file name for the query results',  required=False, default="query_results.csv")
    parser.add_argument('-o', '--on_disk', action='store_false', help='Whether to perform on disk operations',  required=False)
    parser.add_argument('-mw', '--mw_range', nargs="+", type=int, help="The MW upper and/or the lower limit whn searching SMILES", default=None, required=False)
    parser.add_argument('-hac', '--hac_limits', nargs="+", type=int, help="The HAC upper and/or lower limit when searching SMILES", default=None, required=False)
    parser.add_argument("-st", "--search_type", required=False, default="similarity", choices=("similarity", "substructure"), help="The type of search to perform")
    parser.add_argument("-ca", "--commercially_avaliable", action="store_true", help="Whether to only retrieve isomers from commercially avaliable databases", required=False)
    parser.add_argument("-cd", "--commercial_databases", nargs="+", type=str, help="The commercial databases to retrieve isomers from if --commercially_avaliable is set", default=["enamine", "wuxi", "mcule", "molport"], required=False)
    parser.add_argument('-p','--pairwise_database', type=str, help='The folder path for the pairwise database', required=False, default='Molecular_database/pairwise_stats')
    parser.add_argument('-db', '--db_id', type=json.loads, 
                        help='Dictionary mapping commercial database names to their IDs', required=False, 
                        default={"enamine": "001", "wuxi": "014", "mcule": "006", "molport": "013"})
    parser.add_argument("-s", "--stage", choices=("search", "retrieve"), default="search", help="Runing search or retrieve")
    args = parser.parse_args()
    return args.db_name, args.molecular_database, args.index_file, args.top_k, args.threshold, args.num_workers, args.output_file, args.query_path, args.on_disk, args.hac_limits, args.mw_range, args.search_type, args.original_database, args.commercially_avaliable, args.commercial_databases, args.pairwise_database, args.db_id, args.stage


@dataclass
class FPSim2Query:
    db_name: str
    query: str | list[str] # has to be SMILES
    workers: int=4
    on_disk: bool=True
    
    @property
    def fpe(self) -> FPSim2Engine:
        return FPSim2Engine(self.db_name, in_memory_fps=False if self.on_disk else True)
    
    @property
    def queries(self) -> list[str]:
        if isinstance(self.query, str):
            return [self.query]
        return self.query

    def similarity_search(
        self,
        top_k: int=500,
        threshold: float = 0.7):
        """Perform similarity search using FPSIM2"""
        search, tanimoto = {}, {}

        method = (
        self.fpe.on_disk_top_k
        if self.on_disk
        else self.fpe.top_k
        )
        
        for que in self.queries:
            results = method(que, top_k=top_k, threshold=threshold, n_workers=self.workers)
            search[que] = results
        return search

    def substructure_screenout(
        self):
        """Perform substructure search using FPSIM2"""
        
        search = {}
        
        method = (
        self.fpe.on_disk_substructure
        if self.on_disk
        else self.fpe.substructure
        )
        for que in self.queries:
            results = method(que, n_workers=self.workers)
            search[que] = [int(x[0]) for x in results]

        return search


@dataclass
class RetrieveSmiles:
    """
    Retrieve smiles based on indexes from FPSIM2 search results and filter based on HAC or molecular weight range.
    
    Parameters
    -----------
    index_file: str
        The index file generated by deduplication which tells the size of each HAC
    database_path: str
        The folder path for the analoguous search database
    mw_range: Iterable[float] | None = None
        The range of molecular weights to filter. The first number is the lower bound and the second is the upper bound
    hac_limits: Iterable[int] | None = None
        The range of HACs to filter. The first number is the lower bound and the second is the upper bound.
    
    Only one of mw_range or hac_limits can be specified
    """
    index_file: str
    database_path: str
    mw_range: Sequence[float|int] | None = None
    hac_limits: Sequence[int] | None = None
    
    @property
    def upper(self) -> int | None:
        if self.hac_limits is not None and self.mw_range is not None:
            raise ValueError("Only one of mw_range or hac_limits can be specified")
        elif self.hac_limits is not None:
            return self.hac_limits[0]
        elif self.mw_range is not None:
            return convert_mw_to_hac(self.mw_range[0])
        else:
            return None
        
    @property
    def lower(self) -> int | None:
        if self.hac_limits is not None and len(self.hac_limits) == 2:
            return self.hac_limits[1]
        elif self.mw_range is not None and len(self.mw_range[1]) == 2:
            return convert_mw_to_hac(self.mw_range[1])
        else:
            return None
    

    def _filter_hacs(self, hac: int):
        u, l = True, True
        if self.upper is not None:
            u =  hac <= self.upper
        if self.lower is not None:
            l = hac >= self.lower                            
        return all([u, l])
        

    def find_hac_by_index(self, search_results: dict[str, list[int]]) -> dict[str, list[int]]:
        """
        Given the indices of the search results find in which HAC the molecule is found and reduce the scopre when
        retrieving the SMILES strings
        """
        with open(self.index_file, "r") as st:
            lines = {int(x.strip().split("#")[-1]): int(x.split("#")[0].strip("HAC")) for x in st.readlines()}
        index_dict = {}
        bounds = np.array(sorted(lines.keys()))
        for query, result in search_results.items():
            index = sorted(result)
            i = np.unique(np.searchsorted(bounds, index, side="right"))
            hacs = [lines.get(x) for x in bounds[i]]
            index_dict[query] = hacs
        return index_dict

    def convert_hac_topath(self,
        hac_dict: dict[str, list[int]], 
        ) -> dict[str, list[str]]:
        """
        Convert hac to parquet paths for the duck db read_parquet
        """
        parquet_paths = {}
        for query, hacs in hac_dict.items():
            hacs = filter(self._filter_hacs, hacs)
            parquet_paths[query] = [f"{self.database_path}/HAC_{hac}/*.parquet" for hac in hacs]
        return parquet_paths

    def retrieve_smiles(self,
        db_con: duckdb.DuckDBPyConnection,
        indices: list[int],
        parquet_path: list[str],
        ) -> pd.DataFrame:
        """ 
        Retrieve the SMILES from the databases and convert them into Dataframes
        """
        res = db_con.execute("SELECT nostereo_SMILES, num_ID FROM read_parquet($path) WHERE num_ID IN $indices;", {"path": parquet_path, "indices": indices}).df()
        return res

    def batch_retrieve(
        self,
        search_result: dict[str, list[int]], 
        parquet_paths: dict[str, list[str]],
        more_data: dict[str, pd.Series] | None = None,
        ) -> dict[str, pd.DataFrame]:
        
        #extract the index from the FPSIM2 results and generate the combined path and indices
        """
        Batch retrieve SMILES and db_id from the database using the results of FPSIM2 search and the parquet file paths
        """

        index_dict = {query: sorted(ind) for query, ind in search_result.items()}
        parquet = set()
        index = set()
        for i in parquet_paths.values():
            parquet.update(i)
        for i in index_dict.values():
            index.update(i)
            
        result = {}
        # connect to database and search using the combined indices and parquet files
        db_con = duckdb.connect()   
        res = self.retrieve_smiles(db_con, sorted(index), list(parquet))
        for query, index in index_dict.items():
            dat = res[res["num_ID"].isin(index)]
            if more_data is not None:
                coef = more_data[query]
                filt_coef = coef[coef.index.isin(dat["num_ID"])]
                dat = pd.concat([dat, filt_coef], axis=1)
            result[query] = dat
            
        db_con.close()
        return result
    
    def run(self, 
        search_results: dict[str, list[int]],
        more_data: dict[str, pd.Series] | None = None,
        ) -> pd.DataFrame:
        """
        A convenient function to run the similarity search
        """
        hac_dict = self.find_hac_by_index(search_results)
        parquet_paths = self.convert_hac_topath(hac_dict)
        smiles = self.batch_retrieve(search_results, parquet_paths, more_data)
        return smiles


@dataclass
class IsomerRetriever:
    """
    Retrieve isomers from the original database and optionally
    filter by commercial availability.
    """

    original_database: str
    pairwise_database: str | None = None

    commercially_available: bool = False
    commercial_databases: Sequence[str] = ("enamine", "wuxi", "mcule", "molport")

    db_id: dict[str, str] = field(default_factory=lambda: {
        "enamine": "001",
        "wuxi": "014",
        "mcule": "006",
        "molport": "013",
    })

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------
    @property
    def commercial_db_codes(self) -> tuple[str, ...]:
        return tuple(self.db_id[name] for name in self.commercial_databases)
    
    
    def retrieve_batch(
        self,
        queries: dict[str, pd.DataFrame],
    ) -> dict[str, pd.DataFrame]:
        """
        Main entry point: retrieve isomers for each query.
        """
        db_con = duckdb.connect()

        try:
            nostereo_smiles = list(set(pd.concat(queries.values(), axis=0)["nostereo_SMILES"].to_list()))
            parquet_paths = self._original_parquet_paths(nostereo_smiles)

            all_isomers = self._query_original_database(
                db_con,
                nostereo_smiles,
                parquet_paths,
            )

            if self.commercially_available:
                all_isomers = self._filter_commercial_isomers(
                    db_con,
                    all_isomers,
                )

            return self._group_isomers_by_query(queries, all_isomers)

        finally:
            db_con.close()

    # ------------------------------------------------------------------
    # DuckDB queries
    # ------------------------------------------------------------------

    def _query_original_database(
        self,
        db_con: duckdb.DuckDBPyConnection,
        nostereo_smiles: list[str],
        parquet_paths: list[str],
    ) -> pd.DataFrame:
        """
        Retrieve all isomers from the original database with the nostereo_SMILES.
        """
        query = """
        SELECT
            ID,
            SMILES,
            nostereo_SMILES,
            db_id
        FROM read_parquet($path)
        WHERE nostereo_SMILES IN $smiles;
        """

        return db_con.execute(query, {"path": parquet_paths, "smiles": nostereo_smiles}).df()

    def _query_pairwise_database(
        self,
        db_con: duckdb.DuckDBPyConnection,
        smiles: list[str],
        parquet_paths: list[str],
    ) -> pd.DataFrame:
        """
        Retrieve SMILES that appear in any commercial database
        according to the pairwise database.
        """
        query = """
        SELECT SMILES
        FROM read_parquet($path)
        WHERE
            SMILES IN $smiles
            AND len(list_intersect(
                    string_split(Databases, ','),
                    $db_codes
                )
            ) > 0;
        """

        return db_con.execute(query, {"path": parquet_paths, 
                                      "smiles": smiles, 
                                      "db_codes": self.commercial_db_codes}).df()

    # ------------------------------------------------------------------
    # Commercial filtering logic
    # ------------------------------------------------------------------

    def _filter_commercial_isomers(
        self,
        db_con: duckdb.DuckDBPyConnection,
        isomers: pd.DataFrame,
    ) -> pd.DataFrame:
        """
        Keep:
        - isomers directly from commercial databases
        - non-commercial isomers that have duplicates in commercial DBs
        """

        commercial = isomers[isomers["db_id"].isin(self.commercial_db_codes)]
        non_commercial = isomers[~isomers["db_id"].isin(self.commercial_db_codes)]

        if non_commercial.empty:
            return commercial

        duplicated_non_commercial = self._find_duplicates_in_commercial_dbs(
            db_con,
            non_commercial,
        )

        return pd.concat([commercial, duplicated_non_commercial], axis=0)

    def _find_duplicates_in_commercial_dbs(
        self,
        db_con: duckdb.DuckDBPyConnection,
        non_commercial: pd.DataFrame,
    ) -> pd.DataFrame:
        """
        From non-commercial isomers, keep those that exist in
        commercial databases according to the pairwise DB.
        """
        smiles = non_commercial["SMILES"].tolist()
        hacs = self._compute_hacs(smiles)

        parquet_paths = [
            f"{self.pairwise_database}/{hac}/*.parquet"
            for hac in set(hacs)
        ]

        pairwise_hits = self._query_pairwise_database(
            db_con,
            smiles,
            parquet_paths,
        )

        return non_commercial[
            non_commercial["SMILES"].isin(pairwise_hits["SMILES"])
        ]

    # ------------------------------------------------------------------
    # Helpers
    # ------------------------------------------------------------------

    def _original_parquet_paths(self, smiles: list[str]) -> list[str]:
        hacs = self._compute_hacs(smiles)
        return [
            f"{self.original_database}/HAC_{hac}/*.parquet"
            for hac in set(hacs)
        ]

    def _compute_hacs(self, smiles: list[str]) -> list[int]:
        return [
            dm.to_mol(smi).GetNumHeavyAtoms()
            for smi in smiles
        ]

    def _group_isomers_by_query(
        self,
        queries: dict[str, pd.DataFrame],
        all_isomers: pd.DataFrame,
    ) -> dict[str, pd.DataFrame]:
        """
        Return isomers grouped per original query.
        """
        result = {}

        for query, df in queries.items():
            subset = all_isomers[
                all_isomers["nostereo_SMILES"].isin(df["nostereo_SMILES"])
            ].drop(columns=["nostereo_SMILES"])

            result[query] = subset

        return result


def _match_smiles(smi: str, query_mol: dm.Mol) -> bool:
    mol = dm.to_mol(smi)
    return mol.HasSubstructMatch(query_mol)

def match_substructure(queries: str | list[str],
                        df_dict: dict[str, pd.DataFrame],
                        workers: int = 4
                        ) -> dict[str, pd.DataFrame]:
    """Perform substructure matching to filter false positives"""
    subs = {}
    if isinstance(queries, str):
        queries = [queries]
    for que in queries:
        query_mol = dm.to_mol(que)
        mask = dm.parallelized(partial(_match_smiles, query_mol=query_mol), 
                               df_dict[que]["SMILES"], n_jobs=workers, 
                               progress=False, scheduler="threads")
        
        subs[que] = df_dict[que][mask]
        
    return subs


def process_query_by_db(db_name: str, query: str | list[str], 
                        num_workers: int=50, 
                        on_disk: bool = True, 
                        top_k: int =100, 
                        threshold: float = 0.7, 
                        search_type: str = "similarity"):
    
    task_id = int(os.environ.get('SLURM_ARRAY_TASK_ID', 0))
    array_size = int(os.environ.get('SLURM_ARRAY_TASK_COUNT', 1))
    
    my_chunk = list(Path(db_name).glob("*.h5"))[task_id::array_size]
    outpath = Path("search_results")
    outpath.mkdir(exist_ok=True)
    
    for db in my_chunk:
        db_task = int(db.stem.split('_')[-1]) 
        fp = FPSim2Query(query=query, db_name=db, workers=num_workers, on_disk=on_disk)
        search = {"similarity": partial(fp.similarity_search, top_k=top_k, threshold=threshold), 
                "substructure": fp.substructure_screenout}    
        
        search_results = search[search_type]()
        with open(f"{outpath}/search_{db_task}.pkl", "wb") as js:
            pk.dump(search_results, js)
        
        del fp; gc.collect()

def read_search_results(top_k: int = 100, 
                        search_type: str = "similarity") -> dict[str, list[tuple[int, int]]]:
    outpath = Path("search_results")
    if not outpath.exists():
        raise FileNotFoundError("The folder for the search results doesn't exists, check the folder")
    
    search_results = defaultdict(list)
    for x in outpath.glob("*.pkl"):
        with open(x, "rb") as js:
            res = pk.load(js) # A dictionary of {query: search results}
        for query, item in res.items():
            search_results[query].extend(item)
    if search_type == "similarity":
        return {query: sorted(item, key=lambda x: x[2], reverse=True)[:top_k]}
    
    return {query: sorted(item)}

def main():
    db_name, molecular_database, index_file, top_k, threshold, num_workers, output_file, query_path, on_disk,hac_limits, mw_range, search_type, original_database, commercially_avaliable, commercial_databases, pairwise_database, db_id, stage = parse_args()
   
    with open(query_path) as w:
        query = [x.strip() for x in w.readlines()]
        
    match stage:    
        case "search":
            process_query_by_db(db_name, query, num_workers, on_disk, top_k, threshold, search_type)
            
        case "retrieve": 
            search_results = read_search_results(top_k, search_type)
            retrieve = RetrieveSmiles(index_file, molecular_database, mw_range, hac_limits)
            smiles = retrieve.run(search_results)
            if search_type == "substructure":
                smiles = match_substructure(smiles)
            retrieve_isomers = IsomerRetriever(original_database, pairwise_database, commercially_avaliable, commercial_databases, db_id)
            
            smiles = retrieve_isomers.retrieve_batch(smiles)
            Path(output_file).parents.mkdir(parents=True, exist_ok=True)
            pd.concat(smiles).to_csv(output_file)
            
        case _:
            raise NotImplemented("Unknown stage")
    
if __name__ == "__main__":
    # Run this if this file is executed from command line but not if is imported as API
    main()