from FPSim2 import FPSim2Engine
import numpy as np
import argparse
import duckdb
import pandas as pd
from pathlib import Path


def parse_args():
    parser = argparse.ArgumentParser(description='Deduplicate SMILES')
    parser.add_argument('-db', '--db_name', type=str, help='the name of the FPSIM2 database',  required=True)
    parser.add_argument('-mol','--molecular_database', type=str, help='The folder path for the database', required=True, default='Molecular_database')
    parser.add_argument('-q','--query_path', type=str, help='FIle for the query in .smi format', required=True)
    parser.add_argument('-k','--top_k', type=int, help='How many molecular to retrieve (it might return less because of the algorithm)', required=False, default=100)
    parser.add_argument('-t','--threshold', type=float, help='The similarity threshold, faster when higher but it might return less molecules than specified', required=False, default=0.7)
    parser.add_argument('-n','--num_workers', type=int, help='The number of workers for the FPSIM2 search', required=False, default=100)
    parser.add_argument('-i', '--index_file', type=str, help='The index file generated by deduplication',  required=True)
    parser.add_argument('-c', '--return_cols', nargs="+", help='The columns to return together with the smiles',  required=False, default=("SMILES", "num_ID", "db_id"))
    parser.add_argument('-id', '--id_col', type=str, help='The column name to search using the results from FPSIM2',  required=False, default="num_ID")
    parser.add_argument('-c', '--output_file', type=str, help='The csv file name for the query results',  required=False, default="query_results.csv")
    
    args = parser.parse_args()
    return args.db_name, args.molecular_database, args.index_file, args.top_k, args.threshold, args.num_workers, args.return_cols, args.id_col , args.output_file, args.query_path


def querying(
    db_name:str, 
    query: str | list[str], 
    top_k: int=100,
    threshold: float = 0.5,
    workers=4):
    
    fpe = FPSim2Engine(db_name)
    search = {}
    if isinstance(query, str):
        query = [query]
    for que in query: 
        results = fpe.top_k(query, k=top_k, threshold=threshold, metric='tanimoto', n_workers=workers)
        search[que] = results
        
    return search


def find_hac_by_index(index_file: str, search_results: dict[str, np.array]):
    with open(index_file, "r") as st:
        lines = {int(x.strip().split("#")[-1]): int(x.split("#")[0].strip("HAC")) for x in st.readlines()}
    index_dict = {}
    bounds = np.array(lines.keys())
    for query, result in search_results.items():
        index = sorted([x[0] for x in result])
        i = np.unique(np.searchsorted(bounds, index, side="right"))
        hacs = [lines.get(x) for x in bounds[i]]
        index_dict[query] = hacs
    return index_dict

def convert_hac_topath(
    hac_dict: dict[str, str], 
    database_path: str):
    parquet_paths = {}
    for query, hac in hac_dict.items():
        parquet_paths[query] = f"{database_path}/HAC_{hac}/*.parquet"
    return parquet_paths

def retrieve_smiles(
    duck_con,
    indices: list[int],
    parquet_path: list[str],
    return_cols: list[str]=["SMILES", "db_id", "num_ID"],
    id_col="num_ID"
    ):
    
    return_cols = ",".join(return_cols)
    res = duck_con.execute(f"SELECT {return_cols} FROM read_parquet({parquet_path}) WHERE {id_col} IN {indices})").df()
    return res

def batch_retrieve(
    search_result: dict[str, int], 
    parquet_paths: dict[str, str],
    return_cols: list[str]=["SMILES", "db_id", "num_ID"],
    id_col="num_ID"
    ):
    
    #extract the index from the FPSIM2 results and generate the combined path and indices
    index_dict = {query: sorted([u[0] for u in ind]) for query, ind in search_result.items()}
    parquet = set()
    index = set()
    for i in parquet_paths.values():
        parquet.update(i)
    for i in index_dict.values():
        index.update(i)
     
    # connect to database and search using the combined indices and parquet files   
    db_con = duckdb.connect()
    res = retrieve_smiles(db_con, sorted(index), list(parquet), return_cols, id_col)
    result = {}
    for query, index in index_dict.items():
        result[query] = res[res[id_col].isin(index)]
    
    return result


def main():
    db_name, molecular_database, index_file, top_k, threshold, num_workers, return_cols, id_col, output_file, query_path = parse_args()
   
    with open(query_path) as w:
        query = [x.strip() for x in w.readlines()]

    search_results = querying(db_name, query, top_k, threshold, num_workers)
    hac_dict = find_hac_by_index(index_file, search_results)
    parquet_paths = convert_hac_topath(hac_dict, molecular_database)
    smiles = batch_retrieve(search_results, parquet_paths, return_cols, id_col)
    Path(output_file).parents.mkdir(parents=True, exist_ok=True)
    pd.concat(smiles).to_csv(output_file)
    
    
if __name__ == "__main__":
    # Run this if this file is executed from command line but not if is imported as API
    main()